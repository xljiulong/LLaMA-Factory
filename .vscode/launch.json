{
    // Use IntelliSense to learn about possible attributes.
    // Hover to view descriptions of existing attributes.
    // For more information, visit: https://go.microsoft.com/fwlink/?linkid=830387
    "version": "0.2.0",
    "configurations": [
        {
            "name": "debug_ppo_lora",
            "type": "python",
            "request": "launch",
            // "program": "/workspace/projects/LLaMA-Factory/src/train_bash.py",
            "program": "/opt/conda/bin/deepspeed",
            "console": "integratedTerminal",
            "justMyCode": false,
            "args": [
                "--master_port",
                "9527",
                "/workspace/projects/LLaMA-Factory/src/train_bash.py",
                "--stage",
                "ppo",
                "--deepspeed",
                "/workspace/projects/LLaMA-Factory/config/ds_config.json",
                "--do_train", 
                "--model_name_or_path",
                "/workspace/models/huggingface/chatglm3-6b",
                "--adapter_name_or_path",
                "/workspace/models/huggingface/chatglm3-6b_exp_sft_lora_llamafactory",
                "--create_new_adapter",
                "--dataset",
                "alpaca_gpt4_zh",
                "--dataset_dir",
                "/workspace/projects/LLaMA-Factory/data",
                "--template",
                "chatglm3",
                "--finetuning_type",
                "lora",
                "--lora_target",
                "query_key_value",
                "--reward_model",
                "/workspace/models/huggingface/chatglm3-6b_exp_rm_lora_llamafactory", 
                "--output_dir",
                "/workspace/models/huggingface/chatglm3-6b_debug_ppo_lora_llamafactory",
                "--per_device_train_batch_size",
                "2",
                "--gradient_accumulation_steps",
                "4",
                "--lr_scheduler_type",
                "cosine",
                "--top_k",
                "0",
                "--top_p",
                "0.9",
                "--logging_steps",
                "10",
                "--save_steps",
                "1000",
                "--learning_rate",
                "1e-5",
                "--num_train_epochs",
                "1.0",
                "--plot_loss",
                "--fp16"
            ],
            "env": {"CUDA_VISIBLE_DEVICES":"0", "NCCL_P2P_DISABLE":"1", "NCCL_IB_DISABLE":"1", "WANDB_DISABLED":"1"}
        },
        {
            "name": "debug_rm_lora",
            "type": "python",
            "request": "launch",
            // "program": "/workspace/projects/LLaMA-Factory/src/train_bash.py",
            "program": "/opt/conda/bin/deepspeed",
            "console": "integratedTerminal",
            "justMyCode": false,
            "args": [
                "--master_port",
                "9527",
                "/workspace/projects/LLaMA-Factory/src/train_bash.py",
                "--stage",
                "rm",
                "--deepspeed",
                "/workspace/projects/LLaMA-Factory/config/ds_config.json",
                "--do_train", 
                "--model_name_or_path",
                "/workspace/models/huggingface/chatglm3-6b",
                "--adapter_name_or_path",
                "/workspace/models/huggingface/chatglm3-6b_exp_sft_lora_llamafactory",
                "--create_new_adapter",
                "--dataset",
                "comparison_gpt4_zh",
                "--dataset_dir",
                "/workspace/projects/LLaMA-Factory/data",
                "--template",
                "chatglm3",
                "--finetuning_type",
                "lora",
                "--lora_target",
                "query_key_value",
                "--output_dir",
                "/workspace/models/huggingface/chatglm3-6b_debug_rm_lora_llamafactory",
                "--overwrite_cache",
                "--per_device_train_batch_size",
                "2",
                "--gradient_accumulation_steps",
                "4",
                "--lr_scheduler_type",
                "cosine",
                "--logging_steps",
                "10",
                "--save_steps",
                "1000",
                "--learning_rate",
                "1e-6",
                "--num_train_epochs",
                "1.0",
                "--plot_loss",
                "--fp16"
            ],
            "env": {"CUDA_VISIBLE_DEVICES":"0", "NCCL_P2P_DISABLE":"1", "NCCL_IB_DISABLE":"1", "WANDB_DISABLED":"1"}
        },
        {
            "name": "debug_sft_lora",
            "type": "python",
            "request": "launch",
            // "program": "/workspace/projects/LLaMA-Factory/src/train_bash.py",
            "program": "/opt/conda/bin/deepspeed",
            "console": "integratedTerminal",
            "justMyCode": false,
            "args": [
                "--master_port",
                "9527",
                "/workspace/projects/LLaMA-Factory/src/train_bash.py",
                "--stage",
                "sft",
                "--deepspeed",
                "/workspace/projects/LLaMA-Factory/examples/deepspeed/ds_z3_offload_config.json",
                "--do_train", 
                "--model_name_or_path",
                "/workspace/models/huggingface/chatglm3-6b",
                "--dataset",
                "alpaca_gpt4_zh",
                "--dataset_dir",
                "/workspace/projects/LLaMA-Factory/data",
                "--template",
                "chatglm3",
                "--finetuning_type",
                "lora",
                "--lora_target",
                "query_key_value",
                "--output_dir",
                "/workspace/models/huggingface/chatglm3-6b_check_llamafactory",
                "--overwrite_cache",
                "--overwrite_output_dir",
                "--cutoff_len",
                "1024",
                "--preprocessing_num_workers",
                "16",
                "--per_device_train_batch_size",
                "1",
                "--per_device_eval_batch_size",
                "1",
                "--gradient_accumulation_steps",
                "8",
                "--lr_scheduler_type",
                "cosine",
                "--logging_steps",
                "10",
                "--warmup_steps",
                "20",
                "--save_steps",
                "100",
                "-eval_steps",
                "100",
                "--evaluation_strategy",
                "steps",
                "--load_best_model_at_end",
                "--learning_rate",
                "5e-5",
                "--num_train_epochs",
                "3.0",
                "--max_samples",
                "3000",
                "--plot_loss",
                "--fp16"
            ],
            "env": {"CUDA_VISIBLE_DEVICES":"0", "NCCL_P2P_DISABLE":"1", "NCCL_IB_DISABLE":"1", "WANDB_DISABLED":"1"}
        },
        {
            "name": "Python: Current File",
            "type": "python",
            "request": "launch",
            "program": "${file}",
            "console": "integratedTerminal",
            "justMyCode": true
        }
    ]
}