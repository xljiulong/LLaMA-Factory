# 预训练 
## 预训练类别 
## prefix 

## causal 

## 预训练方法 
### transformer 
encoder - decoder 结构 

### bert 
transformer encoder 结构 

### KAN


### RetroMAE
1）重构任务必须对编码质量提出足够的要求； (ENCODER:MASK 15% - 30%; DECODER:50% - 70%)
2）需要充分利用预训练数据。

```
https://blog.csdn.net/m0_57290240/article/details/136195957
```

## 集成学习
## MOE
> https://zhuanlan.zhihu.com/p/672712751
> https://zhuanlan.zhihu.com/p/673048264

##  困惑度
困惑度一般来说是用来评价语言模型好坏的指标。语言模型是衡量句子好坏的模型，本质上是计算句子的概率。
$$ PP(w) = P(w_1, w_2,...,w_n)^{-{1 \over n}} $$

## 交叉熵  

## 对比学习  

# 解码

# 微调

# rlhf

## RM 
> https://zhuanlan.zhihu.com/p/613315873

## 优化方法
### adam
> https://sota.jiqizhixin.com/models/methods/2898844e-4607-489d-99bd-cd3ebb43ff7e

## PPO

## DPO
1. 损失函数
$$
L_{DPO} (\pi) = 
$$  
> https://zhuanlan.zhihu.com/p/688583797  

## ORPO  
> https://zhuanlan.zhihu.com/p/688583797  

# 下游任务

# rag
## 召回
## 生成问答
## 引用生成

# 模型裁剪  
```
https://github.com/yangjianxin1/LLMPruner/tree/master
```

# LLAMA
## GQA
```
https://towardsdatascience.com/demystifying-gqa-grouped-query-attention-3fb97b678e4a  
https://medium.com/@maxshapp/grouped-query-attention-gqa-explained-with-code-e56ee2a1df5a
```

## CROSS ATTENTION  


## activate
### relu
$$ f(x) = max(0, x) $$
### silu
$$ SiLU(x) = x \cdot sigmoid(x)$$  
其中:
$$
Swish(x) = x \cdot sigmoid(\beta x)
$$
> https://himanshuxd.medium.com/activation-functions-sigmoid-relu-leaky-relu-and-softmax-basics-for-neural-networks-and-deep-8d9c70eed91e

# 排序

## 损失
> https://zhuanlan.zhihu.com/p/136199536

## 不同方法
## LambdaRank
> https://zhuanlan.zhihu.com/p/270608987

## 不同损失
### Hinge loss

> https://blog.csdn.net/china1000/article/details/126925750

### GBDT
#### boosting
> https://blog.csdn.net/haveanybody/article/details/110058707

### svm

### bagging,boosting和stacking