# 预训练
## 预训练类别
## prefix

## causal

## 预训练方法
### transformer
encoder - decoder 结构

### bert
transformer encoder 结构 

### RetroMAE
1）重构任务必须对编码质量提出足够的要求； (ENCODER:MASK 15% - 30%; DECODER:50% - 70%)
2）需要充分利用预训练数据。

```
https://blog.csdn.net/m0_57290240/article/details/136195957
```

## 集成学习
## MOE

##  困惑度
困惑度一般来说是用来评价语言模型好坏的指标。语言模型是衡量句子好坏的模型，本质上是计算句子的概率。
$$ PP(w) = P(w_1, w_2,...,w_n)^{-{1 \over n}} $$

## 交叉熵  

## 对比学习  

# 解码

# 微调

# rlhf
## orpo

# 下游任务

# rag
## 召回
## 生成问答
## 引用生成

# 模型裁剪  
```
https://github.com/yangjianxin1/LLMPruner/tree/master
```

# LLAMA
## GQA
```
https://towardsdatascience.com/demystifying-gqa-grouped-query-attention-3fb97b678e4a
```

## CROSS ATTENTION  


## activate

