version: '3'
services:
  llama-factory:
    build:
      context: ../../
      dockerfile: docker/docker-cuda-12.2/dockerfile
    image: llama_factory:0.0.1
    runtime: nvidia
    container_name: llama_factory
    tty: true
    restart: always
    ulimits:
      memlock: -1
      stack: 67108864
    shm_size: 40G
    volumes:
      # - /home/zhangjl19/models/huggingface:/root/.cache/huggingface/
      - /home/zhangjl/snlp:/workspace
    networks:
      - test-net
      
networks:
  test-net:
    external: true